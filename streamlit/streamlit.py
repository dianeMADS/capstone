# -*- coding: utf-8 -*-
"""streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PNepn-yeKr3KDAexRLmP0zaBsaichYbp
"""

import streamlit as st
import pandas as pd
import numpy as np

import gzip
import os

from PIL import Image

import matplotlib.pyplot as plt
import altair as alt
from streamlit_folium import folium_static
import folium
import seaborn as sb

from statsmodels.tsa.api import VAR
from statsmodels.tsa.vector_ar.var_model import VARResults, VARResultsWrapper

# from scikit-learn.metrics import mean_squared_error
# from scikit-learn.metrics import mean_absolute_error

 
# # Suppress warnings
# import warnings
# from statsmodels.tools.sm_exceptions import ValueWarning
# warnings.simplefilter("ignore", ValueWarning) 
# Title

# st.title("MADS 697/698 - CAPSTONE")
# st.header("Diane O and James N")


def main():
  # "# MADS CAPSTONE, SIADS 697-698"
  st.header("MADS CAPSTONE, SIADS 697-698")
  # st.markdown('\n\n')
  st.title("Covid-19 Lockdown and Pollution in the City of Chicago")
  # st.markdown('\n\n')
  st.header("Diane O. and James N.")
  st.markdown('\n\n')

  # menu = ["Test 1", "Test 2"]
  # choice = st.sidebar.selectbox('Menu', menu)
  # if choice == 'Test 1':
  #     st.header("This is Test 1")

  st.markdown(
    """For our project, we explored the Chicago Array of Things (AoT) Dataset.  This massive dataset was collected between 2018 and 2020 from a node array installed throughout the city of Chicago.  Nodes have a variety of sensors installed at each location.  These sensors fall into one of six categories air quality, meteorological, physical, environmental, system, and vision.  Initially, we hoped to focus on determining the quality of life around the city.  This proved to be a bit too ambitious, however.  We found it very difficult to define quality of life and focus in on key measurements.  Instead, we focus mainly on the air quality metrics.  These metrics are easier to track over time and there are clear definitions for gas concentrations that are considered dangerous or unhealthy.  The analysis focuses on the concentrations of five gasses: CO, H2S, NO2, O3, and SO2, as well as oxidizing and reducing gas concentrations."""
  )

  st.markdown('\n\n')
  st.header("I. Reducing the Data")
  st.markdown(
      """The data was initially collected and stored in a column style database with seven columns: timestamp, node id, subsystem, sensor, parameter, raw value, hrf value (processed value).  Ideally, we would have hosted this data on a cloud server and maintained the column database format by using Cassandra or a similar database schema or migrated to a relational database format such as PostgreSQL, however, the dataset was over 360GB in size and pushed the cost too high for our project.  We instead elected to write our own program to reduce the file to a manageable size. """
  )
  st.markdown(
      """After the file was un tar ed to a gzip file, the first step was to split the data into separate files based on the node that collected the data.  This was done using the bash command line and the function below:"""
  )
  st.code(
      """zcat file.csv.gz | awk -F “,” ‘{print>“subfolder/”$2“.csv”}’""", language="cli"
  )
  st.markdown(
      """This command unzips the file line by line and pipes it to the awk function.  This function splits the line by a comma and prints the line to a file with the name of value found in the second column (the node id).  This significantly reduce the size of each file we worked with but with the largest files around 10 GB, we still had to reduce the data.  Data was collected every second at each node.  We did not need this level of granularity and decided to average the readings every hour.  Theoretically, this would reduce our data by a factor of about 3600.  We completed this reduction in C# due to its speed and support for parallel processes, but a similar pipeline could be implemented in python. """
  )
  st.markdown(
      """The process we used is only possible with the data was already sorted by ascending date.  The entire code can be found on our GitHub repository.   The process was to first assign a thread to read a file.  The first line in the file becomes the start time rounded to the nearest half hour rounded down (ex 4:15 would have a start time at 3:30 and 4:45 would have a start time of 4:30) and the end time is the start time plus one hour.  A dictionary of values with the parameter, subsensor, and system as the key and the total for each as the values was then created.  Once the timestamp is greater than the end time or there are no more lines in the file to read, the thread takes the mean value for each key and appends it to a new csv file with the node id.  The values are cleared and the thread continues if there are more values in the file or moves on to the next file.  The entire process takes about 30 minutes to run, and we were able to reduce the size of the entire dataset from 360 GB down to 1.8 GB.  This size is near the limits for pandas but proved to be manageable after filtering by metrics for each analysis."""
  )

  st.subheader("Sample Data from One Single Node")
  sample_data = pd.read_csv('streamlit/data/Numeric_001e06112e77.csv')
  st.dataframe(sample_data.head())

  st.subheader('Master Dataframe (daily aggregation, and human-readable hrf values only)')
  master_df = pd.read_csv("streamlit/data/cleaned_dataset.zip")
  st.dataframe(master_df.head())

  st.subheader('Nodes Generic Information')
  nodes = pd.read_csv("streamlit/data/nodes.csv")
  st.dataframe(nodes.head())

  st.subheader('Sensors Generic Information')
  sensors = pd.read_csv('streamlit/data/sensors.csv')
  st.dataframe(sensors.head())

  st.markdown('\n\n')
  st.header("II. Data Exploration")
  st.markdown(
      """Once the data was reduced to hourly readings, we were able to explore the data using pandas.  Our initial exploration provided us with locations of the sensors and the time frame that each sensor was active.  With this information, we decided to modify our initial plan of focusing on quality of life to focus instead on a causal inference model comparing how trends in air quality changed due to COVID-19 related lockdowns in the city."""
  )
  
  st.subheader('Node Locations')
  st.write(
  """The first step of our data exploration was to look at the node distribution across the city. The map below shows node locations based on their latitude and longitude. The coverage is a very good across the city of Chicago, with a good chunk of nodes along the coast of Lake Michigan. This exercise suggests some clustering for the upcoming analysis; we actually expect air quality to better around the lake than it is within the inner city, especially around industrial zones."""
  )
  latlon = list(zip(nodes['lat'], nodes['lon'], nodes['node_id']))
  mapit = folium.Map( location=[41.85, -87.65], zoom_start=11, width = 600, height = 600)

  for coord in latlon:
    folium.Marker( location=[ coord[0], coord[1] ],
                tooltip=('node:', coord[2], 'lat:', coord[0], 'lon:', coord[1]),
                #  tooltip = ''
                popup=coord[2]).add_to( mapit )

  folium.TileLayer('cartodbpositron').add_to(mapit)
  folium_static(mapit)


  st.markdown('\n\n')
  st.subheader('Collection Period of Time')
  st.write(
      """The city of Chicago AoT website reported that nodes were commissioned and decommissioned between 2017 and 2020. Further than commissioning & decommissioning times, the following visualization indicates when exactly data started and ended to be collected for given nodes. We notice lots of inconsistencies among nodes for related time periods. Recording times went from 0 to 1112 days for an average of 416 days. We also see that nodes were turned up/off at different times within the years under consideration, without specific rules. It is difficult to envisage a per node study, otherwise many (if not most) nodes will not have enough data for a decent analysis in the time before covid-19 (i.e. from the commissioning to March 20th, 2020) and during lockdown from March 21st to May 31st, 2020."""
  )
  st.markdown(
    """i. The two main take-ons from these observations brought us to average parameters over nodes, assuming that parameter values shouldbe similar within the same city, in the same period of time & season."""
  )
  st.markdown(
    """ii. In a second step, we will refine the study to average and thereby make the analysis per similarity or within each cluster."""
  )

  up_df = pd.DataFrame(columns=['node_id', 'start', 'end'])
  idx = 0
  for node in nodes.node_id:
    sample = master_df[master_df.node_id == node]
    up_df.loc[idx] = [node, pd.to_datetime(sample.date).min(), pd.to_datetime(sample.date).max()]
    idx += 1

  up_df['days_up'] = (pd.to_datetime(up_df.end.dt.date) - pd.to_datetime(up_df.start.dt.date)).dt.days
#   st.dataframe(up_df.head())

#   st.dataframe(up_df.describe().T)  

  base = alt.Chart(up_df).encode(
      alt.X('node_id:N')
  ).properties(width = 1200, height = 400)

  rule = base.mark_rule().encode(
      alt.Y('start:T', axis = alt.Axis(format='%m/%y', title='Date')), #,labelAngle=-45
      alt.Y2('end:T')
  )

  startpoints = base.mark_circle(size=60).encode(
      alt.Y('start:T'),
      # alt.Y2('end:T')
  )

  endpoints = base.mark_circle(size=60).encode(
      # alt.Y('start:T'),
      alt.Y('end:T'), color = alt.value("#FFAA00")
  )

  st.altair_chart(rule + startpoints + endpoints)


  st.markdown('\n\n')
  st.subheader('Subsystem Types')
  st.markdown(
    """Another important aspect of the city of Chicago AoT is the set of sensor subsystems present within every node, and which might differ from one node to the other. The visualization below reveals ten different types of subsystems, and indicates whether the subsystems in questiob are present in the nodes for Chicago AoT. Almost all nodes (118 over the 119) comprise lightsense and meltsense subsystems for the collection of meteorological and a few environmental & physical data. Out of those 118 nodes, 94 contain a chemsense to record air quality gaze concentrations along with a couple of meteorological and physical data; note that the only node without lightsense or metsense has a chemsense. On the other hand, the visualization shows that most of the nodes (but three) that have an alphasense do not have a plantower and vice versa. Both plantower and alphasense subsystems collect air quality particle counts; our assumption is those are two versions/releases of the same type of subsystem."""
  )
  st.markdown(
    """Remaining susbystems (i.e. audio, image, ep, nc and wagman) are available in a few nodes only. The amount of image and audio sensors reporting such important metrics as traffic (e.g. cars or people counts) and noise was insufficient to envisage some quality of life definition around those metrics. Wagman, nc and ep subsystems belong to device and network quality measurements, which are out of the scope of this study."""
  )

  subsystem_types = master_df[['node_id', 'subsystem']].groupby(['node_id', 'subsystem']).count().reset_index()
  subsystem_types['count'] = 1


  sensor_chart = alt.Chart(subsystem_types).mark_tick().encode(
    x='node_id',
    y='subsystem',
    color='subsystem'
  ).properties(width=1200, height=400) #
  st.altair_chart(sensor_chart)


  st.markdown('\n\n')
  st.subheader('Sensor Metrics')
  st.markdown(
    """The visualization below is the result of further AoT nodes exploration. It shows actual metrics collected per sensor for the selected set of subsystems. The six subsystems that were selected in previous steps are color-coded, y-axis identifies sensors and x-axis gives corresponding parameters. Pollution-related parameters are gaze concentrations from the seven collected by chemsense, and particles measured by plantower & alphasense. Although meteorological, environmental and physical factors can somehow influence the pollution, our causal inference analysis is limited to air quality attributes for preserving simplicity in this first effort with such a challenging dataset."""
  )

  subsystem_sensor_types = master_df[['subsystem', 'sensor']].groupby(['subsystem', 'sensor']).count().reset_index()
  subsystem_types['count'] = 1

#   subsystem_chart = alt.Chart(subsystem_sensor_types).mark_rect().encode(
#     x='sensor',
#     y='subsystem',
#     # color='subsystem'
#   ).properties(width=1200)
#   st.altair_chart(subsystem_chart)

  filtered_subsystems = master_df[master_df['subsystem'].isin(['lightsense', 'metsense', 'chemsense', 'alphasense', 'plantower'])]
  subsystem_sensor_types = filtered_subsystems[['subsystem', 'sensor']].groupby(['subsystem', 'sensor']).count().reset_index()
  subsystem_types['count'] = 1

#   filteredsub_chart = alt.Chart(subsystem_sensor_types).mark_rect().encode(
#     x='sensor',
#     y='subsystem',
#     color='subsystem'
#   ).properties(width=1400)
#   st.altair_chart(filteredsub_chart)

  sensor_types_parameters = filtered_subsystems[['subsystem', 'sensor', 'parameters']].groupby(['subsystem', 'sensor', 'parameters']).count().reset_index()
  sensor_types_parameters['count'] = 1

  param_chart = alt.Chart(sensor_types_parameters).mark_rect().encode(
    x='parameters',
    y='sensor',
    color='subsystem'
  ).properties(width=1200, height=600)
  st.altair_chart(param_chart)


  st.markdown('\n\n')
  st.subheader('Particle Counts')
  st.markdown(
    """The previous visualization highlights many metrics for particle counts from alphasense and plantower that can be aggregated under three attributes pm1, pm25 and pm10. In this effort of bringing together the measurements from both subsystems, the table below first provides a description of related time series. This brings to the attention a difference of scale between measurements of the two types of sensor subsystems; for example, compare pm1 values that are from alphasense with 1um_particle, pm1_atm and pm1_cf1 collected by plantower."""
  )

#   st.dataframe(filtered_subsystems.head())

  pms = ['10um_particle', '1um_particle', '2_5um_particle', '5um_particle', 'pm1', 'pm10', 'pm10_atm', 'pm10_cf1', 'pm1_atm', 'pm1_cf1', 'pm25_atm', 'pm25_cf1', 'pm2_5', 'point_3um_particle', 'point_5um_particle', 'fw', 'sample_flow_rate', 'sampling_period']
  # 'concentration', 
  df_w_pms = filtered_subsystems[filtered_subsystems['parameters'].isin(pms) ].drop(['node_id', 'subsystem', 'sensor'], axis=1)

  df_w_pms = pd.pivot_table(df_w_pms, values = 'values', index = 'date', columns = 'parameters', aggfunc=np.mean).reset_index()
  # df_w_pms = df_w_pms.fillna(method="bfill")

  # df_w_pms
  st.dataframe(df_w_pms.describe())

  st.markdown('\n\n')
  st.markdown(
    """A quick look at the Literature did not provide a clue to bring the two subsystem measurements to the same scale. The lack of correlation between the different time series prevented us to dig deeper into a strategy to aggregate data from the two subsystems. The heatmap below illustrates our point, with very low correlation between pm1, pm25 and pm10 from alphasense vs plantower measurements. This is the reason why we also eliminate, from now on, particle counts from our study. Pollution is strictly characterized by air quality gazes in the following analysis."""
  )
  st.image(Image.open("streamlit/data/Heatmap_Corr.JPG"), width=1200)

#   "# might need a static image for seaborn and refer back to the notebook..."
#   plt.rc('figure', figsize=(25, 10))
#   fig = sb.heatmap(df_w_pms.corr(method='pearson'), cmap='YlGnBu', annot=True)
#   st.plt(fig)


  st.markdown('\n\n')
  st.subheader('Initial clustering')
  st.markdown(
      """We also ran an initial clustering analysis of the nodes based on the types of sensors at each location. A sparse matrix for each node with the sensor types as the values was used.  The sensor types were limited to only the subsystem types that include air quality metrics.  The histogram below shows the clusters identified using agglomerative clustering with a distance threshold of 15.  """
  )

  st.markdown(
     '''The clustering shows that the concentrations of the gasses are well represented in the blue and red cluster groups.  The particle sizes are only represented in the red cluster group and appear at less than 30 nodes. Therefore, these metrics may not be appropriate for this analysis.  The image below shows the locations of these clusters.  We can see that group 1, represented in orange, covers much of the city but lacks the air quality sensors.  We still believe that the other clusters provide enough coverage of the city and will provide valid results.'''
    )

  st.markdown('\n\n')
  st.image(Image.open("streamlit/data/ExploratoryClusterCounts.jpg"), width=1200)
    

  st.header("III. Causal Inference Analysis")

  st.markdown(
    """This causal inference analysis is to determine if the first lockdown, from March 21st to May 31st 2020, due to COVID-19 rising cases in the city of Chicago reduced pollution within the city. As discussed in data wrangling section, pollution will be determine on the basis of air quality gazes only, because of the limitations of AoT dataset on hands. Air quality gazes are co, h2s, no2, o3, oxydizing gazes, reducing gazes and so2."""
  )
    
  st.markdown(
    """On the other hand, we do not have two (control vs treatment) groups for the causal analysis since the study focuses on the sole city of Chicago, which cannot be simultaneously opened up and lockdown. In such a case, counterfactuals are required to determine what would have happened if the unseen circumstance existed."""
  )

  
  st.subheader('Data')
  st.markdown(
      """The following table shows a sample of the slice of data used for causal inference. Those are pivoted around parameters to have air quality gazes as features, and measurements as observations. The second next table complements the previous one with some statistics around air gaze timeseries."""
  )
  df = filtered_subsystems[filtered_subsystems['parameters'] == 'concentration'].drop(['node_id', 'subsystem', 'parameters'], axis=1)
#   st.dataframe(df.head())
  df = pd.pivot_table(df, values = 'values', index = 'date', columns = 'sensor', aggfunc=np.mean).reset_index()
  # df = df.fillna(method="bfill")
  st.dataframe(df.head())
    
  st.markdown('\n\n')
  st.dataframe(df.describe())

  st.markdown('\n\n')
  st.markdown(
    """For illustration, here are also some plots showing historical trends for air quality gaze concentrations. Gaze concentration is really volatile, with frequent peaks that correspond, from our understanding, to events that a given sensor does not properly function."""
  )
  df.date = pd.to_datetime(df.date)

  base = alt.Chart(df).mark_line().encode(x = 'date:T').properties(width=225, height=225)#.interactive()
  chart = alt.vconcat()

  row = alt.hconcat()
  for gase in ['co:Q', 'h2s:Q', 'no2:Q', 'o3:Q']:
    row |= base.encode(y=gase)
  chart &= row

  row = alt.hconcat()
  for gase in ['oxidizing_gases:Q', 'reducing_gases:Q', 'so2:Q']:
    row |= base.encode(y=gase)
  chart &= row

  st.altair_chart(chart)
 

  st.subheader("""VAR Model""")
  st.markdown(
      """We would like to use pre-lockdown (i.e. before March 21st, 2020) measurements to predict values during lockdown. But before proceeding, it is important to validate the prediction model to be used. We temporarily reserve a small portion of the training set, i.e. January 1st to March 20th 2020, while keeping anything before January 2020 strictly for training."""
  )
  st.markdown(
    """On the other hand, there are seven features to predict, one for each type of gaze; the features in question are timeseries and are expected to be somehow correlated to each other. Vector autoregression (VAR) method fits such mandate pretty well and is quite simple to implement, with a single hyper-parameter p (the causal inference notebook provides details on p tuning in our case). The visualization below plots in orange VAR predicted values for the validation set, against actual measurements in blue."""
  )
   
  training_df = df[df['date'] < '2020-01-01']
#   training_df
  validation_df = df[(df['date'] >= '2020-01-01') & (df['date'] < '2020-03-21')]
#   validation_df

  def var_first_diff(df, p, num_forecasts):
    """
    Fits a VAR(p) model on the first-order diff on a df and makes num_forecasts forecasts
    """
    var_res, forecasts = None, None
    
    data = df.diff().dropna()
    var_res = VAR(data).fit(p)
    lag_order = var_res.k_ar

    #forecasts = var_res.forecast(df.values[-lag_order:], steps=num_forecasts)
    forecasts = pd.DataFrame(var_res.forecast(data.values[-lag_order:], steps=num_forecasts), 
                             columns=df.columns, index=[df.index[-1] + pd.DateOffset(i) for i in range(1, num_forecasts+1)])
    
    # print(forecasts.iloc[0])
    forecasts.iloc[0] += df.iloc[-1]
    for i in range(1, num_forecasts):
        forecasts.iloc[i] += forecasts.iloc[i-1]
    
    return var_res, forecasts

#   param = []
#   accuracy = []

#   best_p = 1

#   best_accuracy = mean_absolute_error(validation_df[['co', 'h2s', 'no2', 'o3', 'oxidizing_gases', 'reducing_gases', 'so2']], 
#                      var_first_diff(training_df.set_index('date'), best_p, 80)[1].reset_index().rename(columns={'index':'date'})[['co', 'h2s', 'no2', 'o3', 'oxidizing_gases', 'reducing_gases', 'so2']])

#   for p in range(1, 50):
#     param.append(p)
  
#     accuracy.append(mean_absolute_error(validation_df[['co', 'h2s', 'no2', 'o3', 'oxidizing_gases', 'reducing_gases', 'so2']], 
#                      var_first_diff(training_df.set_index('date'), p, 80)[1].reset_index().rename(columns={'index':'date'})[['co', 'h2s', 'no2', 'o3', 'oxidizing_gases', 'reducing_gases', 'so2']]))
  
#     temp_accuracy = accuracy[-1]
#     if temp_accuracy <= best_accuracy:
#         best_accuracy = temp_accuracy
#         best_p = p

# #   print(best_p)

#   tunning_chart = alt.Chart(pd.DataFrame(list(zip(param, accuracy)), columns=['p', 'MAE'])).mark_line().encode(
#     x = 'p',
#     y = 'MAE'
#   )
#   st.altair_chart(tunning_chart)

  pred_df = var_first_diff(training_df.set_index('date'), 20, 80)[1].reset_index().rename(columns={'index':'date'})
#   pred_df

  plot_df = validation_df.copy()
  plot_df['type'] = 'actual'
  pred_df['type'] = 'predictions'
  plot_df = plot_df._append(pred_df)

  base = alt.Chart(plot_df).mark_line().encode(x = 'date:T', color='type').properties(width=225, height=225)

  chart = alt.vconcat()

  row = alt.hconcat()
  for gase in ['co:Q', 'h2s:Q', 'no2:Q', 'o3:Q']:
    row |= base.encode(y=gase)
  chart &= row

  row = alt.hconcat()
  for gase in ['oxidizing_gases:Q', 'reducing_gases:Q', 'so2:Q']:
    row |= base.encode(y=gase)
  chart &= row

  st.altair_chart(chart)

 
  st.subheader("""Causal Analysis""")
  st.markdown(
    """We can now use the same pipeline to train our VAR model over the entire period before lockdown on March 21st, 2020. Then we use the fitted model to predict air quality gaze concentrations during lockdown from March 21st to May 31st 2020. Results are shown below with predictions in blue, which represent what would have happened between March 21st and May 31st 2020 without lockdown. The comparison of no-treatment predictions with actual lockdown measurements in orange shows orange values below blue ones for most gazes. This suggests an improvement of air quality gazes during lockdown. Interestingly, so2 concentration does not follow the logic; litterature indicates that so2 mainly results from home pollution (e.g. gazes from oven, emitted while cooking) on the contrary of the other six gazes. So with people being mostly at home during lockdown it is understandable that so2 did not improve during lockdown."""
  )
  st.markdown(
    """Even though the results obtained suggest some air quality improvement during city lockdown, it might be wise to conduct some statistical error analysis or get more robust predictions to support this conclusion. AoT data for air quality gaze concentrations are really volatile; our attempt to add confidence intervals around forecast values need more refining as shown in the notebook. We also believe that a deep learning model like LSTM would provide more robust predictions given the nature of the data."""
  )
 

  control_df = var_first_diff(df[df['date'] < '2020-03-21'].set_index('date'), 20, 72)[1].reset_index().rename(columns={'index':'date'})
#   control_df    
    
  treatment_df = df[(df['date'] >= '2020-03-21') & (df['date'] < '2020-06-01')]
#   treatment_df


  causal_df = treatment_df.copy()
  causal_df['type'] = 'treatment'
  control_df['type'] = 'control'
  causal_df = causal_df._append(control_df)

  base = alt.Chart(causal_df).mark_line().encode(x = 'date:T', color='type').properties(width=225, height=225)

  chart = alt.vconcat()

  row = alt.hconcat()
  for gase in ['co:Q', 'h2s:Q', 'no2:Q', 'o3:Q']:
    row |= base.encode(y=gase)
  chart &= row

  row = alt.hconcat()
  for gase in ['oxidizing_gases:Q', 'reducing_gases:Q', 'so2:Q']:
    row |= base.encode(y=gase)
  chart &= row

  st.altair_chart(chart)
  

  st.markdown('\n\n')
  st.header("IV. Cluster Analysis")
  
  st.markdown(
      """Node clustering seemed like an obvious choice for analysis with similar data being collected around the city.  We were expecting to find some similarity between nodes with each metric that may indicate differences or similarities between neighborhoods that are not physically close to each other.  The focus of this analysis is on the air quality metrics based on the concentrations of selected gasses.  This analysis does not explore what is “good” vs “bad” air quality, it instead focuses on identifying similar concentration levels of gasses over time. Our primary idea was to verify if the conclusions remain the same when applying our causal inference model per cluster, knowing that some areas of the city may have better gaze concentration than others under normal circumstances. The results of this analysis may also be interesting to correlate with other metrics such as income maps, housing prices, zoning (ex. business vs residential areas), and heath maps."""
  )
  st.markdown(
      """Two representation for the data were used to compare results.  The first representation averages the data over each day of the week.  The motivation for this comes from the assumption that there are differences in behavior on each weekday that may influence concentrations of gasses in the air such as differences between workday and weekend traffic (shape (number of nodes, 7 x parameters)).  The second representation averages all the values by date, combining the date in previous years together (shape = (number of nodes, 366 x parameters)). Both representations do not include data collected during the lockdowns to prevent any lockdown related fluctuations from skewing the results.  Both data representations have their benefits.  The weekly representation reduces the daily noise significantly but loses much of the trends over the year that may be important for cluster separation.  The date representation maintains much of the trends but will place more weight on noisy data.  This representation also gives null values more weight since they must be treated as zeros."""
  )
  st.markdown(
      """Agglomerative clustering was used as our baseline algorithm to compare the other methods to.  This method provides the most direct control over the groupings and is the easiest to modify.  Due to the large daily fluctuations, we defined the number of clusters to be 5 instead of tuning for the distance threshold.  DBSCAN and OPTICS rely on identifying groupings of clusters.  These algorithms also identify noise points that are between clusters.  DBSCAN proved extremely difficult to generate reliable clusters.  Most of the runs produced a single cluster and noise points.  OPTICS is a generalized version of DBSCAN and was able to provide better results.  The spectral clustering algorithm had two clusters specified with the radial basis function as the affinity matrix.  Finally, affinity propagation was run as it attempts to determine how close two nodes are to being copies. This algorithm used the preset values suggested by the sklearn documentation with Euclidean distance in the affinity matrix."""
  )
  st.markdown(
      """The results of this analysis were not as consistent as we had hoped.  The most reliable results came from the Agglomerative clustering with the date averaging data representation.  The plot below shows the clusters for the date representation.  The agglomerative clustering algorithm is the only one that appears to have some consistency in its clusters.  DBSCAN was run using a wide range of values for the eps parameter but it was never able to identify more than one cluster and noise.  OPTICS consistently provided a noise group and one or two clusters.  There appears to be a difference between the two clusters in the timeseries, but it is not clear why the separation occurred.  Both groups appear to have significant noise and inconsistent measurement across the timeframe.  The spectral clustering algorithm continually failed to provide fully connected graphs and thus produced unreliable cluster groups.  Affinity propagation produced 22 clusters with both data representations.  It is unclear why so many groups were produced but some differences in the active timeframes for each node and spikes in gas concentrations may account for some of the variability. """
  )
   
  def cluster_timeseries():
    data = pd.read_csv('streamlit/data/clustered_dataset2.zip')
    cols = ['concentration_co', 'concentration_h2s', 'concentration_no2', 'concentration_o3', 'concentration_oxidizing_gases', 'concentration_reducing_gases', 'concentration_so2']
    row = None
    column = None
    for i in ['Agglomerative','DBSCAN','OPTICS', 'Spectral','AffinityPropagation']:
      row = None
      for j in cols:
        df = data.groupby(['date', i]).mean(numeric_only=True)[[j]].reset_index()
        c = alt.Chart(df).mark_line().encode(
            x=alt.X('date:T', title=''),
            y=f'{j}:Q',
            color=f'{i}:N'
        ).properties(width=200)
        if row == None:
          row = c 
        else:
          row = row|c
      if column == None:
        column = row
      else:
        column = (column&row).resolve_scale(color='independent')
    return column
  st.altair_chart(cluster_timeseries())
  st.markdown(
      """For the agglomerative clustering, it appears that there are differences in the cluster groups that are clear in the average time series for each cluster especially between 2019 and 2020. The blue (0) cluster group has far greater variance than the orange (4) cluster.  We can also see that the blue cluster does not continue into 2021. In the map, we can see that most of the nodes belong to a single cluster.   The smallest clusters appear to be the least consistent sensors and have many data gaps or were only active for a short period of time. It appears that the clusters may have only separated based on measurement consistency.  Further analysis is required to understand why these nodes are producing noisier data.  It is unclear if this is due to a faulty sensor or if there are local sources of pollution that cause the spikes.  Future projects could compare these clusters to traffic patterns and industrial centers that could be producers.  It would also be beneficial to include more variables such as wind speed and direction that have an impact on the direction of pollution plume movement.  """
  )
  st.image(Image.open('streamlit/data/AgglomerativeClusters.jpg'), caption='Agglomorative Clusters', width=500)

  
  st.header("V. Possible Future Directions")
  st.markdown('\n\n')
  st.markdown(
      """This capstone is an attempt to determine the impact of COVID-19 lockdown on the air quality gaze concentrations, based on the analysis of Chicago AoT data. One of the biggest challenges was the volume of data that required around 400G of drive space for storing both downloaded zipped and later unzipped files. We also used some bash commands to get an initial view of the data and chunk the initial file into a set of 10G files, one per node. The node files were then reduced to a combined size of 1.8GB, using C# for parallelism and speed capabilities in the aggregation of measurements from seconds to hours. Resulting 1.8GB csv was readable into a Pandas dataframe. A through data exploration allowed us to understand date ranges and nested component relationships within the AoT nodes. We noticed some parameters duplicated from one subsystem to another, with mismatch values within the same given node. Project documentation does not provide enough information to reconcile those measurements; this would require digging deeper to the literature. An initial cluster analysis also showed that air quality particle counts were not present in many sensors. At this point, we decided to focus our analysis on air quality gazes that were provided by the same types of sensors."""
  )
  st.markdown('\n\n')
  st.markdown(
      """We conducted a causal analysis on the impact of the first COVID-19 lockdown, from March 21st to May 31st 2020, in the city of Chicago. We relied on counterfactuals and a Vector Autoregressor to predict, based on historical data, different air quality gaze concentrations if there was no shutdown in the prior-mentioned period. The comparison of predicted values with actual (i.e. lockdown) measurements showed an improvement for most gazes but so2. On the contrary to other gazes under consideration, literature indicates that the latter comes from home pollution; recall that people spent most of their time at home during lockdown. Given the apparent node distribution within the city of Chicago, it seemed logic to rerun the causal inference pipeline on a per cluster basis. We conducted compared Agglomerative, DBSCAN, OPTICS, Spectral and Affinity Propagation clustering models, with the goal of identifying similarities between sensors. The most promising results came from Agglomerative clustering."""
  )
  st.markdown('\n\n')
  st.markdown(
      """In the future, it is necessary to refine our statistical error analysis for the Vector Autoregression to support or mitigate the conclusions of our causal analysis. Furthermore, we would like to implement a LSTM deep learning for more robust predictions in the causal inference analysis. We expect LSTM to better fit the volatile nature of air quality gazes observed in our analysis. Regarding clustering, we are curious to re-run the causal analysis pipeline for each of the clusters obtained with one of the algorithms implemented. From another perspective, we generally ignored many features present in the AoT dataset, such as wind and seasonal changes that might have an impact in both clustering and air quality. In future work, we would like to include more meteorological, environmental, and other physical timeseries. With such generalized study, it will be time to compare AoT data to outside metrics such as health, income and/or housing prices that can somehow relate to the quality of life. Finally, there is some background-related interest to completely shift this AoT analysis with different metrics including traffic patterns, noise and vibrations, or even network connectivity."""
  )
    
  
  st.markdown('\n\n')
  st.markdown(
      """It's been a pleasure to work on this capstone."""
  )

  st.subheader("""James""")
  st.markdown(
      """We both participated in the all aspects of the projects, but James took the lead on data reduction & clustering analysis along with related sections of the report, as well as standup and final videos."""
  )
  
  st.subheader("""Diane""")
  st.markdown(
      """Diane proposed the dataset and was prime for data exploration & causal inference analysis along with related parts of the report; she also set up this streamlit blog."""
  )


if __name__ == '__main__':
  main()
